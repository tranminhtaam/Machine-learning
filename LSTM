import math
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, LSTM, Dropout

# Load the data from the provided CSV file
data = pd.read_csv('HPG.csv')

# Convert 'Date' column to datetime format
data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')
data.set_index('Date', inplace=True)

# Ensure relevant columns are numeric
data['Price'] = data['Price'].str.replace(',', '').astype(float)
data['Open'] = data['Open'].str.replace(',', '').astype(float)
data['High'] = data['High'].str.replace(',', '').astype(float)
data['Low'] = data['Low'].str.replace(',', '').astype(float)
data['Vol.'] = data['Vol.'].str.replace('M', '').astype(float)
data['Change %'] = data['Change %'].str.replace('%', '').astype(float)

# Sort the data by date
data = data.sort_index()

# Visualize the closing price history
plt.figure(figsize=(18, 8))
plt.title('Close Price History')
plt.plot(data['Price'])
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price VNĐ (Đ)', fontsize=18)
plt.show()

# Create a new dataframe with only the 'Price' column
price_data = data.filter(['Price'])

# Filter the data for the training period from January 2nd, 2018 to February 28th, 2022
train_data = price_data.loc['2018-01-02':'2022-02-28']
test_data = price_data.loc['2022-03-01':'2023-03-31']

# Convert the dataframe to a numpy array
train_dataset = train_data.values
test_dataset = test_data.values

# Scale the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_train_data = scaler.fit_transform(train_dataset)
scaled_test_data = scaler.transform(test_dataset)

# Create the training dataset
x_train = []
y_train = []
for i in range(60, len(scaled_train_data)):
    x_train.append(scaled_train_data[i - 60:i, 0])
    y_train.append(scaled_train_data[i, 0])
x_train, y_train = np.array(x_train), np.array(y_train)
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))


# Function to build and compile the LSTM model
def build_model(units_1, dropout_1, units_2, dropout_2, units_3):
    model = Sequential()
    model.add(Input(shape=(x_train.shape[1], 1)))
    model.add(LSTM(units=units_1, return_sequences=True))
    model.add(Dropout(rate=dropout_1))
    model.add(LSTM(units=units_2, return_sequences=False))
    model.add(Dropout(rate=dropout_2))
    model.add(Dense(units=units_3))
    model.add(Dense(1))

    model.compile(optimizer='adam', loss='mean_squared_error')
    return model


# Hyperparameter tuning
best_model = None
best_val_loss = float('inf')
best_hyperparameters = {}

for units_1 in [50, 100, 150]:
    for dropout_1 in [0.2, 0.3, 0.4]:
        for units_2 in [50, 100, 150]:
            for dropout_2 in [0.2, 0.3, 0.4]:
                for units_3 in [25, 50, 75]:
                    model = build_model(units_1, dropout_1, units_2, dropout_2, units_3)
                    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)
                    val_loss = min(history.history['val_loss'])
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        best_model = model
                        best_hyperparameters = {
                            'units_1': units_1,
                            'dropout_1': dropout_1,
                            'units_2': units_2,
                            'dropout_2': dropout_2,
                            'units_3': units_3
                        }

print("Best Hyperparameters:", best_hyperparameters)

# Prepare the test data
test_data = np.concatenate((scaled_train_data[-60:], scaled_test_data), axis=0)
x_test = []
for i in range(60, len(test_data)):
    x_test.append(test_data[i - 60:i, 0])
x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

# Get the model predicted price values
predictions = best_model.predict(x_test)
predictions = scaler.inverse_transform(predictions)

# Ensure the lengths match
y_test = test_dataset[:len(predictions)]

# Plot the data
train = price_data.loc['2018-01-02':'2022-02-28']
valid = price_data.loc['2022-03-01':'2023-03-31']
valid = valid[:len(predictions)]  # Make sure the lengths match
valid['Predictions'] = predictions

# Visualize the data
plt.figure(figsize=(18, 8))
plt.title('Model')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price VNĐ (Đ)', fontsize=18)
plt.plot(train['Price'])
plt.plot(valid[['Price', 'Predictions']])
plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')
plt.show()

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f'RMSE: {rmse}')

# Calculate MAPE
mape = mean_absolute_percentage_error(y_test, predictions)
print(f'MAPE: {mape}')

# Calculate MAE
mae = np.mean(np.abs(predictions - y_test))
print(f'MAE: {mae}')

# Calculate R-squared
r2 = r2_score(y_test, predictions)
print(f'R²: {r2}')
